{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunmadhusud/Yolov8_fast-TensorRT-/blob/main/Tensorrt_onnx_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "t3ZTX34KyLnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies. Restart the current session after installing dependencies.\n",
        "!pip install --upgrade --index-url https://pypi.ngc.nvidia.com nvidia-tensorrt\n",
        "!pip install onnx\n",
        "!pip install pycuda --user"
      ],
      "metadata": {
        "id": "2k6JWYHRkrzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add TensorRT  and cuda library path to the environment variable\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/TensorRT-8.4.3.1/lib\n",
        "!export PATH=$PATH:/usr/local/TensorRT-8.4.3.1/bin"
      ],
      "metadata": {
        "id": "cUp-tSlfDdkr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics"
      ],
      "metadata": {
        "id": "5S5Uj4bgnYhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rha7PPR1hAlD",
        "outputId": "0946bad0-364e-4c24-b984-27ec49fd63bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.1.0.dev20240624-py3-none-any.whl (640 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/640.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m634.9/640.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.1/640.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.25.2)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (3.20.3)\n",
            "Installing collected packages: onnxscript\n",
            "Successfully installed onnxscript-0.1.0.dev20240624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-gpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "053FetYPi6rO",
        "outputId": "cd5d0e1d-a739-46de-b066-8b7b5a0c5ce4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.18.0-cp310-cp310-manylinux_2_28_x86_64.whl (199.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.12.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxconverter-common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "60L33ydIQjls",
        "outputId": "bd184027-92fb-435c-fd0a-7eb009fc3e82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.16.1)\n",
            "Collecting onnxconverter-common\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxconverter-common) (24.1)\n",
            "Collecting protobuf>=3.20.2 (from onnx)\n",
            "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnxconverter-common\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnxconverter-common-1.14.0 protobuf-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8da16ff258d744f8aee3902e1f4ea2c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXTTeu1EnzXw",
        "outputId": "d4d47ffe-b6eb-4bc7-b2b3-c9875f623545"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.42 🚀 Python-3.10.12 torch-2.3.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 33.4/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n"
      ],
      "metadata": {
        "id": "FLLjiFM7ycqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import torchvision\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO('/content/yolov8x.pt')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Move the model to the GPU\n",
        "model.model.to(device).eval()\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# optimized_model = torch.compile(model.model,mode=\"reduce-overhead\" )\n",
        "\n"
      ],
      "metadata": {
        "id": "Z6D_aRKYcUtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd39da6-eb27-4b48-aced-ede4dd991343"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x.pt to '/content/yolov8x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 131M/131M [00:00<00:00, 465MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def nms_pytorch(P : torch.tensor ,scores,thresh_iou : float):\n",
        "    \"\"\"\n",
        "    Apply non-maximum suppression to avoid detecting too many\n",
        "    overlapping bounding boxes for a given object.\n",
        "    Args:\n",
        "        boxes: (tensor) The location preds for the image\n",
        "            along with the class predscores, Shape: [num_boxes,5].\n",
        "        thresh_iou: (float) The overlap thresh for suppressing unnecessary boxes.\n",
        "    Returns:\n",
        "        A list of filtered boxes, Shape: [ , 5]\n",
        "    \"\"\"\n",
        "\n",
        "    # we extract coordinates for every\n",
        "    # prediction box present in P\n",
        "    x1 = P[:, 0]\n",
        "    y1 = P[:, 1]\n",
        "    x2 = P[:, 2]\n",
        "    y2 = P[:, 3]\n",
        "\n",
        "    # we extract the confidence scores as well\n",
        "    # scores = P[:, 4]\n",
        "\n",
        "    # calculate area of every block in P\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # sort the prediction boxes in P\n",
        "    # according to their confidence scores\n",
        "    order = scores.argsort()\n",
        "\n",
        "    # initialise an empty list for\n",
        "    # filtered prediction boxes\n",
        "    keep = []\n",
        "\n",
        "\n",
        "    while len(order) > 0:\n",
        "\n",
        "        # extract the index of the\n",
        "        # prediction with highest score\n",
        "        # we call this prediction S\n",
        "        idx = order[-1]\n",
        "\n",
        "        # push S in filtered predictions list\n",
        "        # keep.append(P[idx])\n",
        "        keep.append(idx.item())\n",
        "\n",
        "        # remove S from P\n",
        "        order = order[:-1]\n",
        "\n",
        "        # sanity check\n",
        "        if len(order) == 0:\n",
        "            break\n",
        "\n",
        "        # select coordinates of BBoxes according to\n",
        "        # the indices in order\n",
        "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
        "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
        "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
        "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
        "\n",
        "        # find the coordinates of the intersection boxes\n",
        "        xx1 = torch.max(xx1, x1[idx])\n",
        "        yy1 = torch.max(yy1, y1[idx])\n",
        "        xx2 = torch.min(xx2, x2[idx])\n",
        "        yy2 = torch.min(yy2, y2[idx])\n",
        "\n",
        "        # find height and width of the intersection boxes\n",
        "        w = xx2 - xx1\n",
        "        h = yy2 - yy1\n",
        "\n",
        "        # take max with 0.0 to avoid negative w and h\n",
        "        # due to non-overlapping boxes\n",
        "        w = torch.clamp(w, min=0.0)\n",
        "        h = torch.clamp(h, min=0.0)\n",
        "\n",
        "        # find the intersection area\n",
        "        inter = w*h\n",
        "\n",
        "        # find the areas of BBoxes according the indices in order\n",
        "        rem_areas = torch.index_select(areas, dim = 0, index = order)\n",
        "\n",
        "        # find the union of every prediction T in P\n",
        "        # with the prediction S\n",
        "        # Note that areas[idx] represents area of S\n",
        "        union = (rem_areas - inter) + areas[idx]\n",
        "\n",
        "        # find the IoU of every prediction in P with S\n",
        "        IoU = inter / union\n",
        "\n",
        "        # keep the boxes with IoU less than thresh_iou\n",
        "        mask = IoU < thresh_iou\n",
        "        order = order[mask]\n",
        "\n",
        "    return keep"
      ],
      "metadata": {
        "id": "ZPpHRsJESCRF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "# import onnxruntime as ort\n",
        "from torchvision.ops import nms\n",
        "import urllib.request\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import time\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "class_names_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "                    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "                    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "                    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "                    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "                    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "                    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "                    'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "def pytorch_run(model,image_array):\n",
        "  \"\"\"\n",
        "  Inference using pytorch model\n",
        "  \"\"\"\n",
        "  input_tensor = torch.from_numpy(image_array).transpose(1,3).transpose(2,3)\n",
        "  input_tensor = input_tensor.to(device)\n",
        "  results = model(input_tensor)\n",
        "  return results\n",
        "\n",
        "\n",
        "def draw_boxes(pytorch_results,image_array, video =False):\n",
        "  \"\"\"\n",
        "  The raw output from yolo model contains information about bouding box coordinates and class probabilities.\n",
        "  The first four indices corresponds to bounding box dimensions and the last dimension corresponds to class probabilities.\n",
        "  This function will extract these information, retains bounding box with confidence score above 0.25 and perform an nms suppression to avoid detecting too many\n",
        "  overlapping bounding boxes for a given object.The final output will be an image with bounding boxes drawn with clear labels to it.\n",
        "  \"\"\"\n",
        "\n",
        "  # image_array = np.array(np.repeat(np.expand_dims(np.array(img), axis=0), BATCH_SIZE, axis=0)) * 255\n",
        "  image_array = (image_array* 255).astype(np.uint8)\n",
        "\n",
        "  save_dir = '/content/annotated_images'\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  for batch_idx in range(pytorch_results.shape[0]):\n",
        "      predictions = pytorch_results[batch_idx].permute(1, 0).unsqueeze(0)\n",
        "      bounding_box_predictions = predictions[:, :, :4]\n",
        "      class_probabilities = predictions[:, :, 4:]\n",
        "\n",
        "      # Convert bounding box predictions to standard format\n",
        "      x1 = bounding_box_predictions[:, :, 0] - bounding_box_predictions[:, :, 2] / 2\n",
        "      y1 = bounding_box_predictions[:, :, 1] - bounding_box_predictions[:, :, 3] / 2\n",
        "      x2 = bounding_box_predictions[:, :, 0] + bounding_box_predictions[:, :, 2] / 2\n",
        "      y2 = bounding_box_predictions[:, :, 1] + bounding_box_predictions[:, :, 3] / 2\n",
        "      bounding_boxes = torch.stack([x1, y1, x2, y2], dim=2)\n",
        "\n",
        "      # Apply non-maximum suppression to bounding boxes\n",
        "      confidence_scores = torch.max(class_probabilities, dim=2)[0]\n",
        "      threshold = 0.25\n",
        "      mask = confidence_scores >= threshold\n",
        "      bounding_boxes = bounding_boxes[mask]\n",
        "      class_probabilities = class_probabilities[mask]\n",
        "      confidence_scores = confidence_scores[mask]\n",
        "\n",
        "      # Apply non-maximum suppression to the bounding boxes\n",
        "      nms_indices = nms_pytorch(bounding_boxes, confidence_scores, 0.45)\n",
        "      selected_predictions = torch.cat([bounding_boxes[nms_indices], class_probabilities[nms_indices]], dim=1)\n",
        "\n",
        "      # Extract bounding boxes and class labels for selected predictions\n",
        "      selected_bounding_boxes = selected_predictions[:, :4]\n",
        "      selected_class_probabilities = selected_predictions[:, 4:]\n",
        "      selected_class_labels = torch.argmax(selected_class_probabilities, dim=1)\n",
        "\n",
        "      # Annotate the image with bounding boxes and labels\n",
        "      for i in range(len(selected_bounding_boxes)):\n",
        "          x1, y1, x2, y2 = selected_bounding_boxes[i].int().tolist()\n",
        "          label = class_names_list[selected_class_labels[i].item()]\n",
        "          confidence = confidence_scores[nms_indices[i]].item()\n",
        "          color = (255, 0, 0)  # Red color\n",
        "          thickness = 2\n",
        "          # image_array[batch_idx] = image_array[batch_idx]*255\n",
        "          cv2.rectangle(image_array[batch_idx], (x1, y1), (x2, y2), color, thickness)\n",
        "          cv2.putText(image_array[batch_idx], f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, thickness)\n",
        "\n",
        "      if not video:\n",
        "        # Save the annotated image for the current batch\n",
        "        save_path = os.path.join(save_dir, f'annotated_image_{batch_idx}.jpg')\n",
        "        rgb_image = cv2.cvtColor(image_array[batch_idx], cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(save_path, rgb_image)\n",
        "  if video:\n",
        "    return image_array\n",
        "\n",
        "# Example usage using a sample image:\n",
        "try:\n",
        "    image_url = 'https://ultralytics.com/images/bus.jpg'\n",
        "    image_path = '/content/bus.jpg'\n",
        "    urllib.request.urlretrieve(image_url, image_path)\n",
        "except Exception as e:\n",
        "    print(\"Error downloading image:\", e)\n",
        "\n",
        "\n",
        "image_url = 'https://ultralytics.com/images/bus.jpg'\n",
        "img = resize(io.imread(image_url), (640, 640))\n",
        "image_array = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
        "pytorch_results = pytorch_run(model.model, image_array)\n",
        "draw_boxes(pytorch_results[0], image_array)\n",
        "\n"
      ],
      "metadata": {
        "id": "MSzP616kSIn0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the video file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "vKuXAmofXcur",
        "outputId": "e101220a-ef1f-423b-d838-47d7f6f0ec2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec84ef7c-cd6f-4b2a-a429-b507d35b59ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec84ef7c-cd6f-4b2a-a429-b507d35b59ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video1 (online-video-cutter.com).mp4 to video1 (online-video-cutter.com).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the path of the uploaded video file\n",
        "video_path = next(iter(uploaded))"
      ],
      "metadata": {
        "id": "ANd0eDrmiUGS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_video(model, input_video_path, output_video_path):\n",
        "    \"\"\"\n",
        "    This function extracts the frames from uploaded video, detect the objects and draw bounding boxes with clear labels.\n",
        "    The frames with the bounding box and detected labels are written back to an output video with fps equal\n",
        "    corresponding to average processing time for each frome taken by the model\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)  # Get the FPS of the input video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (640, 640))  # Temporary FPS setting\n",
        "\n",
        "    batch_frames = []\n",
        "    BATCH_SIZE = 1 # Define your batch size\n",
        "    frame_count = 0\n",
        "    total_processing_time = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_resized = cv2.resize(frame, (640, 640))\n",
        "        batch_frames.append(frame_resized)\n",
        "\n",
        "        if len(batch_frames) == BATCH_SIZE:\n",
        "            batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "            batch_tensor = batch_frames.astype(np.float32)\n",
        "\n",
        "            start_time = time.time()\n",
        "            pytorch_results = pytorch_run(model, batch_tensor)\n",
        "            processing_time = time.time() - start_time\n",
        "            total_processing_time += processing_time\n",
        "            frame_count += len(batch_frames)\n",
        "\n",
        "            annotated_frames = draw_boxes(pytorch_results[0], batch_frames, video=True)\n",
        "            for annotated_frame in annotated_frames:\n",
        "                fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "                cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                out.write(annotated_frame)\n",
        "\n",
        "            batch_frames = []\n",
        "\n",
        "    if batch_frames:  # Process remaining frames if any\n",
        "        print(\"length of additional batch frames\", len(batch_frames))\n",
        "        last_frame = batch_frames[-1]\n",
        "        while len(batch_frames) < BATCH_SIZE:\n",
        "            batch_frames.append(last_frame)\n",
        "        batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "        batch_tensor = batch_frames.astype(np.float32)\n",
        "\n",
        "        start_time = time.time()\n",
        "        pytorch_results = pytorch_run(model, batch_tensor)\n",
        "        processing_time = time.time() - start_time\n",
        "        total_processing_time += processing_time\n",
        "        frame_count += len(batch_frames)\n",
        "\n",
        "        annotated_frames = draw_boxes(pytorch_results[0], batch_frames, video=True)\n",
        "        for annotated_frame in annotated_frames:\n",
        "            fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "            cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            out.write(annotated_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Calculate the effective FPS\n",
        "    effective_fps = frame_count / total_processing_time\n",
        "\n",
        "    print(\"effective_fps\",effective_fps )\n",
        "\n",
        "    # Re-encode the video with the correct FPS\n",
        "    temp_video_path = 'temp_output_video.mp4'\n",
        "    cap = cv2.VideoCapture(output_video_path)\n",
        "    out = cv2.VideoWriter(temp_video_path, fourcc, 60, (640, 640))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Ensure the temporary file was created before renaming\n",
        "    if os.path.exists(temp_video_path):\n",
        "        os.remove(output_video_path)\n",
        "        os.rename(temp_video_path, output_video_path)\n",
        "    else:\n",
        "        print(\"Error: temp_output_video.mp4 was not created successfully.\")\n",
        "\n",
        "# Set the ONNX model path\n",
        "onnx_path = 'yolov8x.onnx'  # Path to your ONNX model\n",
        "\n",
        "# Set the output video path\n",
        "output_video_path = 'output_video.mp4'  # Path to save the output video\n",
        "\n",
        "# Process the uploaded video\n",
        "process_video(model.model, video_path, output_video_path)\n",
        "\n",
        "# Download the output video\n",
        "files.download(output_video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Rm1ru5z_FeIu",
        "outputId": "03b187c3-3646-4c22-c1d0-e2c10664935d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37.850615588870355\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_008b0b6b-926a-4655-83d8-b11df96173bd\", \"output_video.mp4\", 30945106)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Onnx Runtime"
      ],
      "metadata": {
        "id": "2g1EvkCEzBTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO('/content/yolov8x.pt')\n",
        "\n",
        "# Create a dummy input tensor with the appropriate input size\n",
        "dummy_input = torch.randn(1, 3, 640, 640)\n",
        "\n",
        "# Ensure the model is on CPU for exporting\n",
        "model.model.to('cpu')\n",
        "model.model.eval()\n",
        "model.model.float()\n",
        "model.model = model.model.fuse()\n",
        "\n",
        "# Export the model to ONNX\n",
        "onnx_path = '/content/yolov8x.onnx'\n",
        "torch.onnx.export(model.model, dummy_input, onnx_path, verbose=True, opset_version=17)\n",
        "\n",
        "print(f\"Model has been successfully exported to {onnx_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoA0N7XdqPKI",
        "outputId": "91a19180-dbd8-4e52-f015-5a4f9195671e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/head.py:92: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.dynamic or self.shape != shape:\n",
            "/usr/local/lib/python3.10/dist-packages/ultralytics/utils/tal.py:299: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  for i, stride in enumerate(strides):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been successfully exported to /content/yolov8x.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "onnx_model = onnx.load(\"/content/yolov8x.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "d3W9KCxVh-QY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\n",
        "onnxruntime.get_available_providers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaUSTzYz6cwO",
        "outputId": "809fa613-2f95-46d1-ca81-e6a3591e6407"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TensorrtExecutionProvider',\n",
              " 'CUDAExecutionProvider',\n",
              " 'AzureExecutionProvider',\n",
              " 'CPUExecutionProvider']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import onnx\n",
        "# from onnxconverter_common import float16\n",
        "\n",
        "# model = onnx.load(\"/content/yolov8x.onnx\")\n",
        "# model_fp16 = float16.convert_float_to_float16(model)\n",
        "# onnx.save(model_fp16, \"/content/yolov8x_fp16.onnx\")"
      ],
      "metadata": {
        "id": "5Ly0nBs-Qw8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import onnx\n",
        "\n",
        "# # Load the original ONNX model\n",
        "# model = onnx.load(\"/content/yolov8x.onnx\")\n",
        "\n",
        "# # Inspect the model's input and output data types\n",
        "# for input in model.graph.input:\n",
        "#     print(f\"Input: {input.name}, Type: {input.type}\")\n",
        "\n",
        "# for output in model.graph.output:\n",
        "#     print(f\"Output: {output.name}, Type: {output.type}\")\n"
      ],
      "metadata": {
        "id": "S6dti9oxEzWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def nms_pytorch(P : torch.tensor ,scores,thresh_iou : float):\n",
        "    \"\"\"\n",
        "    Apply non-maximum suppression to avoid detecting too many\n",
        "    overlapping bounding boxes for a given object.\n",
        "    Args:\n",
        "        boxes: (tensor) The location preds for the image\n",
        "            along with the class predscores, Shape: [num_boxes,5].\n",
        "        thresh_iou: (float) The overlap thresh for suppressing unnecessary boxes.\n",
        "    Returns:\n",
        "        A list of filtered boxes, Shape: [ , 5]\n",
        "    \"\"\"\n",
        "\n",
        "    # we extract coordinates for every\n",
        "    # prediction box present in P\n",
        "    x1 = P[:, 0]\n",
        "    y1 = P[:, 1]\n",
        "    x2 = P[:, 2]\n",
        "    y2 = P[:, 3]\n",
        "\n",
        "    # we extract the confidence scores as well\n",
        "    # scores = P[:, 4]\n",
        "\n",
        "    # calculate area of every block in P\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # sort the prediction boxes in P\n",
        "    # according to their confidence scores\n",
        "    order = scores.argsort()\n",
        "\n",
        "    # initialise an empty list for\n",
        "    # filtered prediction boxes\n",
        "    keep = []\n",
        "\n",
        "\n",
        "    while len(order) > 0:\n",
        "\n",
        "        # extract the index of the\n",
        "        # prediction with highest score\n",
        "        # we call this prediction S\n",
        "        idx = order[-1]\n",
        "\n",
        "        # push S in filtered predictions list\n",
        "        # keep.append(P[idx])\n",
        "        keep.append(idx.item())\n",
        "\n",
        "        # remove S from P\n",
        "        order = order[:-1]\n",
        "\n",
        "        # sanity check\n",
        "        if len(order) == 0:\n",
        "            break\n",
        "\n",
        "        # select coordinates of BBoxes according to\n",
        "        # the indices in order\n",
        "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
        "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
        "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
        "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
        "\n",
        "        # find the coordinates of the intersection boxes\n",
        "        xx1 = torch.max(xx1, x1[idx])\n",
        "        yy1 = torch.max(yy1, y1[idx])\n",
        "        xx2 = torch.min(xx2, x2[idx])\n",
        "        yy2 = torch.min(yy2, y2[idx])\n",
        "\n",
        "        # find height and width of the intersection boxes\n",
        "        w = xx2 - xx1\n",
        "        h = yy2 - yy1\n",
        "\n",
        "        # take max with 0.0 to avoid negative w and h\n",
        "        # due to non-overlapping boxes\n",
        "        w = torch.clamp(w, min=0.0)\n",
        "        h = torch.clamp(h, min=0.0)\n",
        "\n",
        "        # find the intersection area\n",
        "        inter = w*h\n",
        "\n",
        "        # find the areas of BBoxes according the indices in order\n",
        "        rem_areas = torch.index_select(areas, dim = 0, index = order)\n",
        "\n",
        "        # find the union of every prediction T in P\n",
        "        # with the prediction S\n",
        "        # Note that areas[idx] represents area of S\n",
        "        union = (rem_areas - inter) + areas[idx]\n",
        "\n",
        "        # find the IoU of every prediction in P with S\n",
        "        IoU = inter / union\n",
        "\n",
        "        # keep the boxes with IoU less than thresh_iou\n",
        "        mask = IoU < thresh_iou\n",
        "        order = order[mask]\n",
        "\n",
        "    return keep"
      ],
      "metadata": {
        "id": "dlMtb7I1B5tj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import onnxruntime as ort\n",
        "from torchvision.ops import nms\n",
        "import urllib.request\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import time\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "class_names_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "                    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "                    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "                    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "                    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "                    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "                    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "                    'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "def onnx_run(onnx_path,image_array,BATCH_SIZE=1):\n",
        "  \"\"\"\n",
        "  Inference using Onnx runtime\n",
        "  \"\"\"\n",
        "\n",
        "  providers = [(\"CUDAExecutionProvider\", {\"device_id\": torch.cuda.current_device(),\n",
        "                                          \"user_compute_stream\": str(torch.cuda.current_stream().cuda_stream)})]  # Use default CUDA device and stream\n",
        "\n",
        "  sess_options = ort.SessionOptions()\n",
        "  sess_options.log_severity_level = 0\n",
        "  ort_session = ort.InferenceSession(onnx_path, sess_options=sess_options, providers=providers)\n",
        "\n",
        "\n",
        "  # image_array = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
        "  # image_array = np.array(img, dtype=np.float32)\n",
        "  # print(image_array.shape)\n",
        "  input_tensor = np.transpose(image_array, (0, 3, 1, 2))\n",
        "\n",
        "\n",
        "  #IOBinding\n",
        "  input_names = ort_session.get_inputs()[0].name\n",
        "  input_shapes = ort_session.get_inputs()[0].shape\n",
        "  input_type = ort_session.get_inputs()[0].type\n",
        "  # print(input_names, input_shapes, input_type)\n",
        "\n",
        "  output_names = ort_session.get_outputs()[0].name\n",
        "  output_shapes = ort_session.get_outputs()[0].shape\n",
        "  output_type = ort_session.get_outputs()[0].type\n",
        "  # print(output_names, output_shapes, output_type)\n",
        "\n",
        "\n",
        "  io_binding = ort_session.io_binding()\n",
        "  io_binding.bind_cpu_input(input_names, input_tensor)\n",
        "  io_binding.bind_output(output_names, 'cpu')\n",
        "\n",
        "  start_time = time.time()\n",
        "  ort_session.run_with_iobinding(io_binding)\n",
        "  end_time = time.time()\n",
        "  time_elapsed = end_time - start_time\n",
        "  # print(time_elapsed)\n",
        "\n",
        "  ort_outs = io_binding.copy_outputs_to_cpu()\n",
        "  onnx_results = torch.tensor(ort_outs[0])\n",
        "\n",
        "  return onnx_results\n",
        "\n",
        "\n",
        "def draw_boxes(onnx_results,image_array, video =False):\n",
        "  \"\"\"\n",
        "  The raw output from yolo model contains information about bouding box coordinates and class probabilities.\n",
        "  The first four indices corresponds to bounding box dimensions and the last dimension corresponds to class probabilities.\n",
        "  This function will extract these information, retains bounding box with confidence score above 0.25 and perform an nms suppression to avoid detecting too many\n",
        "  overlapping bounding boxes for a given object.The final output will be an image with bounding boxes drawn with clear labels to it.\n",
        "  \"\"\"\n",
        "\n",
        "  # image_array = np.array(np.repeat(np.expand_dims(np.array(img), axis=0), BATCH_SIZE, axis=0)) * 255\n",
        "  image_array = (image_array* 255).astype(np.uint8)\n",
        "\n",
        "  save_dir = '/content/annotated_images'\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  for batch_idx in range(onnx_results.shape[0]):\n",
        "      predictions = onnx_results[batch_idx].permute(1, 0).unsqueeze(0)\n",
        "      bounding_box_predictions = predictions[:, :, :4]\n",
        "      class_probabilities = predictions[:, :, 4:]\n",
        "\n",
        "      # Convert bounding box predictions to standard format\n",
        "      x1 = bounding_box_predictions[:, :, 0] - bounding_box_predictions[:, :, 2] / 2\n",
        "      y1 = bounding_box_predictions[:, :, 1] - bounding_box_predictions[:, :, 3] / 2\n",
        "      x2 = bounding_box_predictions[:, :, 0] + bounding_box_predictions[:, :, 2] / 2\n",
        "      y2 = bounding_box_predictions[:, :, 1] + bounding_box_predictions[:, :, 3] / 2\n",
        "      bounding_boxes = torch.stack([x1, y1, x2, y2], dim=2)\n",
        "\n",
        "      # Apply non-maximum suppression to bounding boxes\n",
        "      confidence_scores = torch.max(class_probabilities, dim=2)[0]\n",
        "      threshold = 0.25\n",
        "      mask = confidence_scores >= threshold\n",
        "      bounding_boxes = bounding_boxes[mask]\n",
        "      class_probabilities = class_probabilities[mask]\n",
        "      confidence_scores = confidence_scores[mask]\n",
        "\n",
        "      # Apply non-maximum suppression to the bounding boxes\n",
        "      nms_indices = nms_pytorch(bounding_boxes, confidence_scores, 0.45)\n",
        "      selected_predictions = torch.cat([bounding_boxes[nms_indices], class_probabilities[nms_indices]], dim=1)\n",
        "\n",
        "      # Extract bounding boxes and class labels for selected predictions\n",
        "      selected_bounding_boxes = selected_predictions[:, :4]\n",
        "      selected_class_probabilities = selected_predictions[:, 4:]\n",
        "      selected_class_labels = torch.argmax(selected_class_probabilities, dim=1)\n",
        "\n",
        "      # Annotate the image with bounding boxes and labels\n",
        "      for i in range(len(selected_bounding_boxes)):\n",
        "          x1, y1, x2, y2 = selected_bounding_boxes[i].int().tolist()\n",
        "          label = class_names_list[selected_class_labels[i].item()]\n",
        "          confidence = confidence_scores[nms_indices[i]].item()\n",
        "          color = (255, 0, 0)  # Red color\n",
        "          thickness = 2\n",
        "          # image_array[batch_idx] = image_array[batch_idx]*255\n",
        "          cv2.rectangle(image_array[batch_idx], (x1, y1), (x2, y2), color, thickness)\n",
        "          cv2.putText(image_array[batch_idx], f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, thickness)\n",
        "\n",
        "      if not video:\n",
        "        # Save the annotated image for the current batch\n",
        "        save_path = os.path.join(save_dir, f'annotated_image_{batch_idx}.jpg')\n",
        "        rgb_image = cv2.cvtColor(image_array[batch_idx], cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(save_path, rgb_image)\n",
        "  if video:\n",
        "    return image_array\n",
        "\n",
        "\n",
        "try:\n",
        "    image_url = 'https://ultralytics.com/images/bus.jpg'\n",
        "    image_path = '/content/bus.jpg'\n",
        "    urllib.request.urlretrieve(image_url, image_path)\n",
        "except Exception as e:\n",
        "    print(\"Error downloading image:\", e)\n",
        "\n",
        "\n",
        "image_url = 'https://ultralytics.com/images/bus.jpg'\n",
        "img = resize(io.imread(image_url), (640, 640))\n",
        "image_array = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
        "\n",
        "onnx_path = '/content/yolov8x.onnx'\n",
        "onnx_results = onnx_run(onnx_path, image_array)\n",
        "draw_boxes(onnx_results, image_array)\n"
      ],
      "metadata": {
        "id": "ViiJyomFbuRA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the video file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the path of the uploaded video file\n",
        "video_path = next(iter(uploaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "vO7gMgacT3L2",
        "outputId": "27de9c59-71f7-4325-919f-698c506072b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6d0a2d28-c506-47fd-97c5-7b3f2ac2a1cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6d0a2d28-c506-47fd-97c5-7b3f2ac2a1cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video1 (online-video-cutter.com)(1).mp4 to video1 (online-video-cutter.com)(1).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(onnx_path, input_video_path, output_video_path):\n",
        "    \"\"\"\n",
        "    This function extracts the frames from uploaded video, detect the objects and draw bounding boxes with clear labels.\n",
        "    The frames with the bounding box and detected labels are written back to an output video with fps equal\n",
        "    corresponding to average processing time for each frome taken by the model\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)  # Get the FPS of the input video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (640, 640))  # Temporary FPS setting\n",
        "\n",
        "    batch_frames = []\n",
        "    BATCH_SIZE = 1 # Define your batch size\n",
        "    frame_count = 0\n",
        "    total_processing_time = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_resized = cv2.resize(frame, (640, 640))\n",
        "        batch_frames.append(frame_resized)\n",
        "\n",
        "        if len(batch_frames) == BATCH_SIZE:\n",
        "            batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "            batch_tensor = batch_frames.astype(np.float32)\n",
        "\n",
        "            start_time = time.time()\n",
        "            # pytorch_results = pytorch_run(model, batch_tensor)\n",
        "            onnx_results = onnx_run(onnx_path, batch_tensor)\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            total_processing_time += processing_time\n",
        "            frame_count += BATCH_SIZE\n",
        "\n",
        "            # annotated_frames = draw_boxes(pytorch_results[0], batch_frames, video=True)\n",
        "            annotated_frames = draw_boxes(onnx_results, batch_frames, video=True)\n",
        "            for annotated_frame in annotated_frames:\n",
        "                fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "                cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                out.write(annotated_frame)\n",
        "\n",
        "            batch_frames = []\n",
        "\n",
        "    if batch_frames:  # Process remaining frames if any\n",
        "        last_frame = batch_frames[-1]\n",
        "        while len(batch_frames) < BATCH_SIZE:\n",
        "            batch_frames.append(last_frame)\n",
        "        batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "        batch_tensor = batch_frames.astype(np.float32)\n",
        "\n",
        "        start_time = time.time()\n",
        "        # pytorch_results = pytorch_run(model, batch_tensor)\n",
        "        onnx_results = onnx_run(onnx_path, batch_tensor)\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        total_processing_time += processing_time\n",
        "        frame_count += len(batch_frames)\n",
        "\n",
        "        # annotated_frames = draw_boxes(pytorch_results[0], batch_frames, video=True)\n",
        "        annotated_frames = draw_boxes(onnx_results, batch_frames, video=True)\n",
        "        for annotated_frame in annotated_frames:\n",
        "            fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "            cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            out.write(annotated_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Calculate the effective FPS\n",
        "    effective_fps = frame_count / total_processing_time\n",
        "\n",
        "    print(effective_fps )\n",
        "\n",
        "    # Re-encode the video with the correct FPS\n",
        "    temp_video_path = 'temp_output_video.mp4'\n",
        "    cap = cv2.VideoCapture(output_video_path)\n",
        "    out = cv2.VideoWriter(temp_video_path, fourcc, 60, (640, 640))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Ensure the temporary file was created before renaming\n",
        "    if os.path.exists(temp_video_path):\n",
        "        os.remove(output_video_path)\n",
        "        os.rename(temp_video_path, output_video_path)\n",
        "    else:\n",
        "        print(\"Error: temp_output_video.mp4 was not created successfully.\")\n",
        "\n",
        "# Set the ONNX model path\n",
        "onnx_path = 'yolov8x.onnx'  # Path to your ONNX model\n",
        "\n",
        "# Set the output video path\n",
        "output_video_path = 'output_video.mp4'  # Path to save the output video\n",
        "\n",
        "# Process the uploaded video\n",
        "process_video(onnx_path, video_path, output_video_path)\n",
        "\n",
        "# Download the output video\n",
        "files.download(output_video_path)"
      ],
      "metadata": {
        "id": "PdAmoS4KogVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d0fc003f-6bd8-4bc8-d189-336e851b512f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.27098718280516293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f0cc1b0-3db3-481c-9233-c068dc79f56b\", \"output_video.mp4\", 2658830)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorRT\n"
      ],
      "metadata": {
        "id": "-XHkfbygz4uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8x.pt\")\n",
        "model.fuse()\n",
        "model.info(verbose=False)\n",
        "model.export(format='onnx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "NSlIuLPogYyk",
        "outputId": "66ca62b2-c51d-4510-f90f-aab7b874866f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n",
            "Ultralytics YOLOv8.2.42 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.00GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8x.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (130.5 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.1 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 10.4s, saved as 'yolov8x.onnx' (260.4 MB)\n",
            "\n",
            "Export complete (18.8s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8x.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8x.onnx imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov8x.onnx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# set the flag to True to run inference in FP16\n",
        "USE_FP16 = True\n",
        "np_target_dtype = np.float32\n",
        "target_dtype = torch.float32\n"
      ],
      "metadata": {
        "id": "ZRBpZnIVpYm4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_filename = \"/content/yolov8x.onnx\"\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Create a logger object with a severity level of INFO.\n",
        "logger = trt.Logger(trt.Logger.INFO)\n",
        "\n",
        "# create a Builder object. This object will be used to build the TensorRT engine from the network definition\n",
        "builder = trt.Builder(logger)\n",
        "\n",
        "#Network will be created with explicit batch size. This means that the batch size of the input tensors-\n",
        "#will be specified explicitly during the engine building process, rather than being determined automatically by TensorRT.\n",
        "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "\n",
        "# Create a parser object that can be used to parse an ONNX model file and-\n",
        "# convert it to a TensorRT network object.\n",
        "parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "# Parse the ONNX model to the TensorRT network\n",
        "with open(onnx_filename, 'rb') as model_file:\n",
        "    if not parser.parse(model_file.read()):\n",
        "        for error in range(parser.num_errors):\n",
        "            print(parser.get_error(error))\n",
        "\n",
        "# Create an optimization profile object that can be used to optimize the TensorRT engine for a specific range of input shapes.\n",
        "profile = builder.create_optimization_profile()\n",
        "\n",
        "#Set the minimum/optimum/maximum dimensions for a dynamic input tensor in the optimization profile object.\n",
        "profile.set_shape('input', (BATCH_SIZE, 3, 640, 640), (BATCH_SIZE, 3, 640, 640), (BATCH_SIZE, 3, 640, 640))\n",
        "\n",
        "#Create a builder configuration object that can be used to configure the TensorRT engine builder\n",
        "config = builder.create_builder_config()\n",
        "\n",
        "# Set the precision to float16 if USE_FP16 is True\n",
        "if USE_FP16:\n",
        "    config.set_flag(trt.BuilderFlag.FP16)\n",
        "    config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)\n",
        "\n",
        "config.add_optimization_profile(profile)\n",
        "\n",
        "max_workspace_size = 1 << 30  # 1GB\n",
        "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)\n",
        "\n",
        "\n",
        "# Build the TensorRT engine from the optimized network [DEPRECATED METHOD]\n",
        "# engine = builder.build_engine(network, config)\n",
        "\n",
        "# Build the serialized TensorRT network blob\n",
        "trt_blob = builder.build_serialized_network(network, config)\n",
        "\n",
        "# Create TensorRT runtime object\n",
        "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
        "\n",
        "# Clean up parser if no longer needed\n",
        "del parser\n",
        "\n",
        "# Deserialize CUDA engine from the serialized blob\n",
        "engine = runtime.deserialize_cuda_engine(trt_blob)\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ8BpwjxprL6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the TensorRT engine to a file\n",
        "\n",
        "# [FOR DEPRECRATED METHOD]\n",
        "# with open('model.trt', 'wb') as engine_file:\n",
        "#     engine_file.write(engine.serialize())\n",
        "\n",
        "# For deserialize_cuda_engine method\n",
        "# Write the serialized blob to a file\n",
        "with open('model.trt', 'wb') as f:\n",
        "    f.write(trt_blob)\n",
        "\n",
        "print(f\"Serialized TensorRT engine saved to {'model.trt'}\")"
      ],
      "metadata": {
        "id": "mcExC36IjroH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f419846-1246-4f58-f47a-d847543c0af2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serialized TensorRT engine saved to model.trt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the TensorRT engine from the saved file\n",
        "%%time\n",
        "\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "\n",
        "# Create TensorRT runtime object\n",
        "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n",
        "\n",
        "# Load the serialized engine from file\n",
        "with open('model.trt', 'rb') as f:\n",
        "    trt_blob = f.read()\n",
        "    engine = runtime.deserialize_cuda_engine(trt_blob)\n",
        "\n",
        "print(f\"Serialized TensorRT engine loaded from {'model.trt'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9OHGIcujuHe",
        "outputId": "8edddfb7-a5ab-4c0f-99f1-657cb4932d52"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serialized TensorRT engine loaded from model.trt\n",
            "CPU times: user 366 ms, sys: 244 ms, total: 610 ms\n",
            "Wall time: 632 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "def nms_pytorch(P : torch.tensor ,scores,thresh_iou : float):\n",
        "    \"\"\"\n",
        "    Apply non-maximum suppression to avoid detecting too many\n",
        "    overlapping bounding boxes for a given object.\n",
        "    Args:\n",
        "        boxes: (tensor) The location preds for the image\n",
        "            along with the class predscores, Shape: [num_boxes,5].\n",
        "        thresh_iou: (float) The overlap thresh for suppressing unnecessary boxes.\n",
        "    Returns:\n",
        "        A list of filtered boxes, Shape: [ , 5]\n",
        "    \"\"\"\n",
        "\n",
        "    # we extract coordinates for every\n",
        "    # prediction box present in P\n",
        "    x1 = P[:, 0]\n",
        "    y1 = P[:, 1]\n",
        "    x2 = P[:, 2]\n",
        "    y2 = P[:, 3]\n",
        "\n",
        "    # we extract the confidence scores as well\n",
        "    # scores = P[:, 4]\n",
        "\n",
        "    # calculate area of every block in P\n",
        "    areas = (x2 - x1) * (y2 - y1)\n",
        "\n",
        "    # sort the prediction boxes in P\n",
        "    # according to their confidence scores\n",
        "    order = scores.argsort()\n",
        "\n",
        "    # initialise an empty list for\n",
        "    # filtered prediction boxes\n",
        "    keep = []\n",
        "\n",
        "\n",
        "    while len(order) > 0:\n",
        "\n",
        "        # extract the index of the\n",
        "        # prediction with highest score\n",
        "        # we call this prediction S\n",
        "        idx = order[-1]\n",
        "\n",
        "        # push S in filtered predictions list\n",
        "        # keep.append(P[idx])\n",
        "        keep.append(idx.item())\n",
        "\n",
        "        # remove S from P\n",
        "        order = order[:-1]\n",
        "\n",
        "        # sanity check\n",
        "        if len(order) == 0:\n",
        "            break\n",
        "\n",
        "        # select coordinates of BBoxes according to\n",
        "        # the indices in order\n",
        "        xx1 = torch.index_select(x1,dim = 0, index = order)\n",
        "        xx2 = torch.index_select(x2,dim = 0, index = order)\n",
        "        yy1 = torch.index_select(y1,dim = 0, index = order)\n",
        "        yy2 = torch.index_select(y2,dim = 0, index = order)\n",
        "\n",
        "        # find the coordinates of the intersection boxes\n",
        "        xx1 = torch.max(xx1, x1[idx])\n",
        "        yy1 = torch.max(yy1, y1[idx])\n",
        "        xx2 = torch.min(xx2, x2[idx])\n",
        "        yy2 = torch.min(yy2, y2[idx])\n",
        "\n",
        "        # find height and width of the intersection boxes\n",
        "        w = xx2 - xx1\n",
        "        h = yy2 - yy1\n",
        "\n",
        "        # take max with 0.0 to avoid negative w and h\n",
        "        # due to non-overlapping boxes\n",
        "        w = torch.clamp(w, min=0.0)\n",
        "        h = torch.clamp(h, min=0.0)\n",
        "\n",
        "        # find the intersection area\n",
        "        inter = w*h\n",
        "\n",
        "        # find the areas of BBoxes according the indices in order\n",
        "        rem_areas = torch.index_select(areas, dim = 0, index = order)\n",
        "\n",
        "        # find the union of every prediction T in P\n",
        "        # with the prediction S\n",
        "        # Note that areas[idx] represents area of S\n",
        "        union = (rem_areas - inter) + areas[idx]\n",
        "\n",
        "        # find the IoU of every prediction in P with S\n",
        "        IoU = inter / union\n",
        "\n",
        "        # keep the boxes with IoU less than thresh_iou\n",
        "        mask = IoU < thresh_iou\n",
        "        order = order[mask]\n",
        "\n",
        "    return keep"
      ],
      "metadata": {
        "id": "JFQ8RaoN3trL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "import os\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import pycuda.driver as cuda\n",
        "\n",
        "class_names_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "                    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "                    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "                    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "                    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "                    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "                    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "                    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "                    'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "# Preprocess the input image\n",
        "def preprocess_image(img):\n",
        "    result = torch.from_numpy(img).transpose(0,2).transpose(1,2)\n",
        "    return np.array(result, dtype=np_target_dtype)\n",
        "\n",
        "\n",
        "\n",
        "def tensorrt_run(image_array):\n",
        "  results = predict(image_array)\n",
        "  return results\n",
        "\n",
        "def predict(batch): # result gets copied into output\n",
        "    # transfer input data (from CPU) to device (GPU) asynchronously\n",
        "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
        "    # execute model on the input data asynchronously\n",
        "    context.execute_async_v2(bindings, stream.handle, None)\n",
        "    # transfer predictions back from the device (GPU) to the host (CPU) asynchronously\n",
        "    cuda.memcpy_dtoh_async(output_data , d_output, stream)\n",
        "    # syncronize threads\n",
        "    stream.synchronize()\n",
        "\n",
        "    return output_data\n",
        "\n",
        "\n",
        "\n",
        "def draw_boxes(tensorrt_results,image_array, video =False):\n",
        "  \"\"\"\n",
        "  The raw output from yolo model contains information about bouding box coordinates and class probabilities.\n",
        "  The first four indices corresponds to bounding box dimensions and the last dimension corresponds to class probabilities.\n",
        "  This function will extract these information, retains bounding box with confidence score above 0.25 and perform an nms suppression to avoid detecting too many\n",
        "  overlapping bounding boxes for a given object.The final output will be an image with bounding boxes drawn with clear labels to it.\n",
        "  \"\"\"\n",
        "\n",
        "  # image_array = np.array(np.repeat(np.expand_dims(np.array(img), axis=0), BATCH_SIZE, axis=0)) * 255\n",
        "  image_array = (image_array* 255).astype(np.uint8)\n",
        "\n",
        "  save_dir = '/content/annotated_images'\n",
        "  os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "  # tensorrt_results = torch.from_numpy(tensorrt_results[batch_idx])\n",
        "\n",
        "  for batch_idx in range(tensorrt_results.shape[0]):\n",
        "      # print(tensorrt_results[batch_idx].shape)\n",
        "      predictions = torch.from_numpy(tensorrt_results[batch_idx]).permute(1, 0).unsqueeze(0)\n",
        "      bounding_box_predictions = predictions[:, :, :4]\n",
        "      class_probabilities = predictions[:, :, 4:]\n",
        "\n",
        "      # Convert bounding box predictions to standard format\n",
        "      x1 = bounding_box_predictions[:, :, 0] - bounding_box_predictions[:, :, 2] / 2\n",
        "      y1 = bounding_box_predictions[:, :, 1] - bounding_box_predictions[:, :, 3] / 2\n",
        "      x2 = bounding_box_predictions[:, :, 0] + bounding_box_predictions[:, :, 2] / 2\n",
        "      y2 = bounding_box_predictions[:, :, 1] + bounding_box_predictions[:, :, 3] / 2\n",
        "      bounding_boxes = torch.stack([x1, y1, x2, y2], dim=2)\n",
        "\n",
        "      # Apply non-maximum suppression to bounding boxes\n",
        "      confidence_scores = torch.max(class_probabilities, dim=2)[0]\n",
        "      threshold = 0.25\n",
        "      mask = confidence_scores >= threshold\n",
        "      bounding_boxes = bounding_boxes[mask]\n",
        "      class_probabilities = class_probabilities[mask]\n",
        "      confidence_scores = confidence_scores[mask]\n",
        "\n",
        "      # Apply non-maximum suppression to the bounding boxes\n",
        "      nms_indices = nms_pytorch(bounding_boxes, confidence_scores, 0.45)\n",
        "      selected_predictions = torch.cat([bounding_boxes[nms_indices], class_probabilities[nms_indices]], dim=1)\n",
        "\n",
        "      # Extract bounding boxes and class labels for selected predictions\n",
        "      selected_bounding_boxes = selected_predictions[:, :4]\n",
        "      selected_class_probabilities = selected_predictions[:, 4:]\n",
        "      selected_class_labels = torch.argmax(selected_class_probabilities, dim=1)\n",
        "\n",
        "      # Annotate the image with bounding boxes and labels\n",
        "      for i in range(len(selected_bounding_boxes)):\n",
        "          x1, y1, x2, y2 = selected_bounding_boxes[i].int().tolist()\n",
        "          label = class_names_list[selected_class_labels[i].item()]\n",
        "          confidence = confidence_scores[nms_indices[i]].item()\n",
        "          color = (255, 0, 0)  # Red color\n",
        "          thickness = 2\n",
        "          # image_array[batch_idx] = image_array[batch_idx]*255\n",
        "          cv2.rectangle(image_array[batch_idx], (x1, y1), (x2, y2), color, thickness)\n",
        "          cv2.putText(image_array[batch_idx], f'{label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, thickness)\n",
        "\n",
        "      if not video:\n",
        "        # Save the annotated image for the current batch\n",
        "        save_path = os.path.join(save_dir, f'annotated_image_{batch_idx}.jpg')\n",
        "        rgb_image = cv2.cvtColor(image_array[batch_idx], cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(save_path, rgb_image)\n",
        "  if video:\n",
        "    return image_array\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "# Download a sample image for inference\n",
        "url='https://ultralytics.com/images/bus.jpg'\n",
        "img = resize(io.imread(url), (640, 640))\n",
        "input_batch = np.array(np.repeat(np.expand_dims(np.array(img, dtype=np.float32), axis=0), BATCH_SIZE, axis=0), dtype=np.float32)\n",
        "\n",
        "preprocessed_images = np.array([preprocess_image(image) for image in input_batch])\n",
        "\n",
        "# Create an execution context for inference using the built TensorRT engine\n",
        "context = engine.create_execution_context()\n",
        "\n",
        "# Allocate space for the output data, with the appropriate shape and data type\n",
        "output_data = np.empty([BATCH_SIZE, 84, 8400], dtype = np_target_dtype)\n",
        "\n",
        "# Allocate device memory for input and output\n",
        "d_input = cuda.mem_alloc(1 * preprocessed_images.nbytes)\n",
        "d_output = cuda.mem_alloc(1 * output_data.nbytes)\n",
        "\n",
        "# Create a list of bindings, which are the addresses of the input and output buffers\n",
        "bindings = [int(d_input), int(d_output)]\n",
        "\n",
        "# Create a CUDA stream for asynchronous execution of TensorRT engine.\n",
        "stream = cuda.Stream()\n",
        "start_time = time.time()\n",
        "tensorrt_results = tensorrt_run(preprocessed_images)\n",
        "processing_time = time.time() - start_time\n",
        "print(\"processing_time\",processing_time )\n",
        "draw_boxes(tensorrt_results,input_batch)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mxlVjPk8P1n8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed537e50-b238-4f68-ee41-f11562b90734"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing_time 0.0275270938873291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the video file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the path of the uploaded video file\n",
        "video_path = next(iter(uploaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "XOwi7aP24794",
        "outputId": "6a5cd33a-2aa3-43e5-f984-bc5186469878"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2eb5b215-7c00-46fd-bc85-57504ac0b754\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2eb5b215-7c00-46fd-bc85-57504ac0b754\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving video1 (online-video-cutter.com).mp4 to video1 (online-video-cutter.com) (1).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import time\n",
        "def process_video(onnx_path, input_video_path, output_video_path):\n",
        "\n",
        "    \"\"\"\n",
        "    This function extracts the frames from uploaded video, detect the objects and draw bounding boxes with clear labels.\n",
        "    The frames with the bounding box and detected labels are written back to an output video with fps equal\n",
        "    corresponding to average processing time for each frome taken by the model\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)  # Get the FPS of the input video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (640, 640))  # Temporary FPS setting\n",
        "\n",
        "    batch_frames = []\n",
        "    BATCH_SIZE = 1 # Define batch size\n",
        "    frame_count = 0\n",
        "    total_processing_time = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_resized = cv2.resize(frame, (640, 640))\n",
        "        batch_frames.append(frame_resized)\n",
        "\n",
        "        if len(batch_frames) == BATCH_SIZE:\n",
        "            batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "            batch_tensor = batch_frames.astype(np.float32)\n",
        "            preprocessed_images = np.array([preprocess_image(image) for image in batch_tensor ])\n",
        "\n",
        "            start_time = time.time()\n",
        "            # pytorch_results = pytorch_run(model, batch_tensor)\n",
        "            # onnx_results = onnx_run(onnx_path, batch_tensor)\n",
        "            tensorrt_results = tensorrt_run(preprocessed_images)\n",
        "            processing_time = time.time() - start_time\n",
        "            # print(\"processing_time\",processing_time )\n",
        "            total_processing_time += processing_time\n",
        "            frame_count += len(batch_frames)\n",
        "\n",
        "            annotated_frames = draw_boxes(tensorrt_results, batch_frames, video=True)\n",
        "            for annotated_frame in annotated_frames:\n",
        "                fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "                cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "                out.write(annotated_frame)\n",
        "\n",
        "            batch_frames = []\n",
        "\n",
        "    if batch_frames:  # Process remaining frames if any\n",
        "        print(\"length of additional batch frames\", len(batch_frames))\n",
        "        last_frame = batch_frames[-1]\n",
        "        while len(batch_frames) < BATCH_SIZE:\n",
        "            batch_frames.append(last_frame)\n",
        "        batch_frames = np.array(batch_frames, dtype=np.float32) / 255.0\n",
        "        batch_tensor = batch_frames.astype(np.float32)\n",
        "\n",
        "        start_time = time.time()\n",
        "        tensorrt_results = tensorrt_run(preprocessed_images)\n",
        "        processing_time = time.time() - start_time\n",
        "        total_processing_time += processing_time\n",
        "        frame_count += len(batch_frames)\n",
        "\n",
        "        annotated_frames = draw_boxes(tensorrt_results, batch_frames, video=True)\n",
        "        for annotated_frame in annotated_frames:\n",
        "            fps_text = f'FPS: {frame_count / total_processing_time:.2f}'\n",
        "            cv2.putText(annotated_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "            out.write(annotated_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Calculate the effective FPS\n",
        "    effective_fps = frame_count / total_processing_time\n",
        "\n",
        "    print(\"effective_fps\",effective_fps )\n",
        "\n",
        "    # Re-encode the video with the correct FPS\n",
        "    temp_video_path = 'temp_output_video.mp4'\n",
        "    cap = cv2.VideoCapture(output_video_path)\n",
        "    out = cv2.VideoWriter(temp_video_path, fourcc, 60, (640, 640))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Ensure the temporary file was created before renaming\n",
        "    if os.path.exists(temp_video_path):\n",
        "        os.remove(output_video_path)\n",
        "        os.rename(temp_video_path, output_video_path)\n",
        "    else:\n",
        "        print(\"Error: temp_output_video.mp4 was not created successfully.\")\n",
        "\n",
        "# Set the ONNX model path\n",
        "onnx_path = 'yolov8x.onnx'  # Path to your ONNX model\n",
        "\n",
        "# Set the output video path\n",
        "output_video_path = 'output_video.mp4'  # Path to save the output video\n",
        "\n",
        "# Process the uploaded video\n",
        "process_video(onnx_path, video_path, output_video_path)\n",
        "\n",
        "# Download the output video\n",
        "files.download(output_video_path)"
      ],
      "metadata": {
        "id": "K08iwk8I4Kyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bb9d761a-1357-4d87-cbd0-5684d8032d05"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "effective_fps 63.3081439967135\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f3196a7b-82ec-4f9d-930a-ffd195edc8f8\", \"output_video.mp4\", 30902813)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}